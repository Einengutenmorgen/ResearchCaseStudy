{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 247\u001b[0m\n\u001b[1;32m    244\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/research/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Desktop/Research/research/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Research/research/lib/python3.10/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "from typing import Union, List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the pipeline\"\"\"\n",
    "    base_url: str = \"http://localhost:1234/v1\"\n",
    "    model_name: str = \"bartowski/Qwen2.5-32B-Instruct-GGUF/Qwen2.5-32B-Instruct-IQ2_M.gguf\"\n",
    "    max_tokens: int = 8000\n",
    "    temperature: float = 0.3\n",
    "    seed: int = 420\n",
    "    chunk_size: int = 1000\n",
    "    max_retries: int = 3\n",
    "    retry_delay: int = 2\n",
    "    stop: List[str] = field(default_factory=lambda: [\"<|im_end|>\"])  # Add stop tokens\n",
    "\n",
    "\n",
    "class DataValidationError(Exception):\n",
    "    \"\"\"Custom exception for data validation errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class LLMError(Exception):\n",
    "    \"\"\"Custom exception for LLM-related errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handles data loading and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_data(self, path: Union[str, Path]) -> pd.DataFrame:\n",
    "        \"\"\"Load data from CSV file and perform initial validation\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            self._validate_dataframe(df)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _validate_dataframe(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Validate required columns and data types\"\"\"\n",
    "        required_columns = ['full_text', 'tweet_id', 'screen_name', 'original_user_id']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise DataValidationError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess the dataframe\"\"\"\n",
    "        df = df.copy()\n",
    "        # Standardize user IDs\n",
    "        df['user_id'] = df['original_user_id'].apply(\n",
    "            lambda x: x[0] if isinstance(x, list) else x\n",
    "        )\n",
    "        # Filter valid entries\n",
    "        df = df.dropna(subset=['full_text', 'user_id'])\n",
    "        return df\n",
    "\n",
    "class LLMFormatter:\n",
    "    \"\"\"Handles prompt formatting for LLM\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_prompt(system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"Format messages for Llama model\"\"\"\n",
    "        return f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_neutral_prompt(post: str) -> str:\n",
    "        \"\"\"Create prompt for neutral post description\"\"\"\n",
    "        return f\"\"\"Create a neutral, high-level description of the following social media post.\n",
    "Focus on the type of content, general topic, and format.\n",
    "Avoid subjective interpretations or unnecessary details.\n",
    "Keep the description clear, concise, and unbiased.\n",
    "\n",
    "Post: {post}\"\"\"\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Handles communication with LLM API\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.formatter = LLMFormatter()\n",
    "        \n",
    "    async def generate_response(\n",
    "        self, \n",
    "        session: ClientSession,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"Generate response from LLM with retry logic\"\"\"\n",
    "        # Format the messages in the expected format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self.formatter.format_prompt(system_prompt, user_prompt)\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                async with session.post(\n",
    "                    f\"{self.config.base_url}/chat/completions\",\n",
    "                    json={\n",
    "                        \"model\": self.config.model_name,\n",
    "                        \"messages\": messages,  # Use messages format\n",
    "                        \"temperature\": self.config.temperature,\n",
    "                        \"seed\": self.config.seed,\n",
    "                        \"max_tokens\": self.config.max_tokens,\n",
    "                        \"stop\": self.config.stop\n",
    "                    }\n",
    "                ) as response:\n",
    "                    if response.status == 200:\n",
    "                        result = await response.json()\n",
    "                        return result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                    else:\n",
    "                        error_text = await response.text()\n",
    "                        logger.error(f\"Attempt {attempt + 1} failed: {error_text}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt + 1} failed with exception: {e}\")\n",
    "                \n",
    "            # Wait before retrying\n",
    "            await asyncio.sleep(self.config.retry_delay ** attempt)\n",
    "            \n",
    "        return None\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Main pipeline class orchestrating the whole process\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.processor = DataProcessor(config)\n",
    "        self.llm_client = LLMClient(config)\n",
    "        \n",
    "    async def process_chunk(\n",
    "        self,\n",
    "        chunk: pd.DataFrame,\n",
    "        session: ClientSession\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a chunk of data\"\"\"\n",
    "        results = []\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                neutral_prompt = LLMFormatter.create_neutral_prompt(row['full_text'])\n",
    "                response = await self.llm_client.generate_response(\n",
    "                    session,\n",
    "                    \"Analyze and describe the post neutrally.\",\n",
    "                    neutral_prompt\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"user_id\": row['user_id'],\n",
    "                    \"original_text\": row['full_text'],\n",
    "                    \"llm_response\": response,\n",
    "                    \"success\": response is not None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row: {e}\")\n",
    "                results.append({\n",
    "                    \"user_id\": row['user_id'],\n",
    "                    \"original_text\": row['full_text'],\n",
    "                    \"llm_response\": str(e),\n",
    "                    \"success\": False\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    async def run(self, input_path: Union[str, Path], output_path: Union[str, Path]):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess data\n",
    "            df = self.processor.load_data(input_path)\n",
    "            df = self.processor.preprocess_data(df)\n",
    "            \n",
    "            all_results = []\n",
    "            \n",
    "            # Process in chunks\n",
    "            async with ClientSession() as session:\n",
    "                for chunk_start in range(0, len(df), self.config.chunk_size):\n",
    "                    chunk = df.iloc[chunk_start:chunk_start + self.config.chunk_size]\n",
    "                    results = await self.process_chunk(chunk, session)\n",
    "                    all_results.extend(results)\n",
    "                    \n",
    "                    # Log progress\n",
    "                    logger.info(f\"Processed {chunk_start + len(chunk)}/{len(df)} rows\")\n",
    "            \n",
    "            # Create results DataFrame\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            \n",
    "            # Save results\n",
    "            results_df.to_parquet(output_path)\n",
    "            logger.info(f\"Results saved to {output_path}\")\n",
    "            \n",
    "            return results_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {e}\")\n",
    "            raise\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(config)\n",
    "    \n",
    "    # Run pipeline\n",
    "    try:\n",
    "        results_df = await pipeline.run(\n",
    "            input_path='/Users/mogen/Desktop/Research/storage/df_test_10k.csv',\n",
    "            output_path='/Users/mogen/Desktop/Research/storage/output.parquet'\n",
    "        )\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "        return results_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Enable nested asyncio for Jupyter notebooks\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    # Run the pipeline\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
