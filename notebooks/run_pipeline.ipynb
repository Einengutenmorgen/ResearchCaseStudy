{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Conversation Pipeline\n",
    "\n",
    "This notebook demonstrates the usage of the conversation pipeline for processing tweet conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to Python path to make src imports work\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Now we can import from src\n",
    "from src.pipeline.conversation_pipeline import ConversationExtractor, Tweet\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess the data\n",
    "def load_tweet_data(file_path):\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    required_columns = ['tweet_id', 'full_text', 'screen_name', 'created_at', \n",
    "                       'reply_to_id', 'reply_to_user', 'expandedURL']\n",
    "    \n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(df)} tweets\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    print(df.columns.tolist())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/mogen/Desktop/Research/data/df_test_10k.csv\n",
      "Successfully loaded 10000 tweets\n",
      "\n",
      "Dataset columns:\n",
      "['full_text', 'tweet_id', 'created_at', 'screen_name', 'original_user_id', 'retweeted_user_ID', 'collected_at', 'reply_to_id', 'reply_to_user', 'expandedURL']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the conversation extractor\n",
    "extractor = ConversationExtractor()\n",
    "\n",
    "# File paths - adjust these as needed\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "test_file = os.path.join(data_dir, 'df_test_10k.csv')\n",
    "full_file = os.path.join(data_dir, 'Kopie von FolloweeIDs2_tweets_df_AugustPull.csv')\n",
    "\n",
    "# Try to load test file first\n",
    "try:\n",
    "    df = load_tweet_data(test_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Test file not found: {test_file}\")\n",
    "    print(\"Trying full dataset...\")\n",
    "    df = load_tweet_data(full_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data in chunks\n",
    "chunk_size = 1000\n",
    "total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "\n",
    "print(f\"Processing {len(df)} tweets in chunks of {chunk_size}\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[i:i + chunk_size]\n",
    "    extractor.process_chunk(chunk)\n",
    "    \n",
    "    # Print progress every chunk\n",
    "    current_chunk = (i // chunk_size) + 1\n",
    "    print(f\"Processed chunk {current_chunk}/{total_chunks} ({(i + len(chunk))} tweets)\")\n",
    "    \n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "stats = extractor.get_stats()\n",
    "print(\"\\nProcessing Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Analyze conversation lengths\n",
    "conversation_lengths = [len(tweets) for tweets in extractor.conversations.values()]\n",
    "conv_stats = pd.Series(conversation_lengths).describe()\n",
    "\n",
    "print(\"\\nConversation Length Statistics:\")\n",
    "print(conv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few sample conversations\n",
    "print(\"Sample Conversations:\")\n",
    "for conv_id, tweets in list(extractor.conversations.items())[:3]:\n",
    "    print(f\"\\nConversation {conv_id} ({len(tweets)} tweets):\")\n",
    "    for tweet in sorted(tweets, key=lambda x: x.timestamp):\n",
    "        print(f\"  {tweet.timestamp}: {tweet.author}: {tweet.text[:100]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
